cd C:\spark-3.3.2-bin-hadoop3\bin

spark-class org.apache.spark.deploy.master.Master

spark-class org.apache.spark.deploy.worker.Worker spark://192.168.56.1:7077 -m 1G -c 2

spark-shell --master {spark://host:port ou yarn} --name TP1-Spark

:help

:type sc --> output : org.apache.spark.SparkContext

:imports

------------------


// 10
var nb = List(0,1,2,3,4,5,6,7,8,9,10)

// 11
val rrdnb = sc.parallelize(nb)
//rrdnb: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

// 12
val rrdnbCollect = rrdnb.collect()
// rrdnbCollect: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
rrdnbCollect.foreach(println)
rrdnbCollect.foreach(e=>println(e))

//13
//????????????

//14
rrdnb.getNumPartitions

//15 (partition du rrd)
rrdnb.glom.collect()

//16
// ???????????

//17
sc.defaultParallelism

//18 
// idem 15

//19
rrdnb.count()
// Long=11

//20
val rrdnbTop3 = rrdnb.take(3)
// take() est une transformation


//21
// lambda expression
val rrdnbMult = rrdnb.map(e=>e*10)
rrdnbMult.collect().foreach(e=>print(e+"\t"))

// fonction nommÃ©e (propre)
def fMultiplier(nb:Int,facteur:Int):Int= {
  return nb*facteur;
}
val rrdnbMult = rrdnb.map(fMultiplier(_,10))
rrdnbMult.collect().foreach(e=>print(e+"\t"))


//22
def estPaire(nb:Int):Boolean = {
  return nb%2==0;	
}

def fModifier (nb:Int, facteur:Int, delta:Int): Int = {
  if (estPaire(nb)) {
    return nb*facteur;
  }
  else {
    return nb+delta;
  }
}

val rrdnbModifier = rrdnb.map(fModifier(_,10,10))
rrdnbModifier.collect().foreach(e=>print(e+"\t"))

// OUT : 0       11      20      13      40      15      60      17      80      19      100

//23
val rrdnbPaire = rrdnb.filter(e=>e%2==0)
rrdnbPaire.collect().foreach(e=>print(e+"\t"))

//24
// idem 21 ????

//25
rrdnb.sum()
// Double = 55.0

//26
rrdnb.filter(e=>estPaire(e)).sum()
rrdnb.filter(e=>e%2==0).sum()
// Double = 30.0

//27
def defineParityKey(nb:Int):(String,Int)= {
  if (nb%2==0) {
    return ("pairs",nb);
  }
  else if (nb%2==1){
   return ("impairs",nb); 
  }
  return ("error",0);
}

val rddSommeCleParite = rrdnb.map(e=>defineParityKey(e)).reduceByKey(_+_)

rddSommeCleParite.collect().foreach(e=>print(e+"\t"))

//28

def defineParityKeyCount(nb:Int):(String,Int)= {
  if (nb%2==0) {
    return ("pairs",1);
  }
  else if (nb%2==1){
   return ("impairs",1); 
  }
  return ("error",0);
}

val rddCompteCleParite = rrdnb.map(e=>defineParityKeyCount(e)).reduceByKey((v1,v2)=>v1+v2)

rddCompteCleParite.collect().foreach(e=>print(e+"\t"))

//29
rrdnb.partitioner

//30 - Re partition
val rddRepartition5 = rrdnb.repartition(5)
rddRepartition5.getNumPartitions
rddRepartition5.glom().collect()
sc.defaultParallelism // ne change pas

val rddRepartition3 = rddRepartition5.coalesce(3)
rddRepartition3.getNumPartitions
rddRepartition3.glom().collect()
sc.defaultParallelism

//31